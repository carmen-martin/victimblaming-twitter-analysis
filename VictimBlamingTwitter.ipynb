{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "072a946c",
   "metadata": {},
   "source": [
    "# Victim Blaming Project \n",
    "\n",
    "# Data collection, data preprocessing and network building\n",
    "\n",
    "#### Author: Carmen Martin Turrero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3fde1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import requests \n",
    "import pandas as pd \n",
    "import time\n",
    "import itertools\n",
    "import networkx as nx\n",
    "import igraph as ig\n",
    "import ast\n",
    "import cairocffi as cairo\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import our functions\n",
    "import download_utils\n",
    "import clean_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456b31a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/content/drive/MyDrive/TwitterData/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e425a2",
   "metadata": {},
   "source": [
    "## Download tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b0e1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose token\n",
    "os.environ['TOKEN'] = \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40b86e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers\n",
    "def create_headers(bearer_token):\n",
    "    headers = {\"Authorization\": \"Bearer {}\".format(bearer_token)}\n",
    "    return headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563aa977",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = create_headers(os.environ['TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a3058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set query\n",
    "lang = 'lang:en'\n",
    "key_hashtags = '#metoo OR #victimblaming OR #rapevictim OR #beingrapedneverreported OR #streetharassment OR #rapeculture OR #slutshame OR #rape OR (#abuse -sibling) OR #sexualabuse OR #harassment OR (#survivor abuse) OR (#survivor rape) OR #sexualharassment'\n",
    "questions = '\"did you try to stop it\" OR \"He ought to have enjoyed it\" OR \"Did you yell\" OR \"Do you have proofs\" OR \"What were you wearing\" OR \"flirting with him\" OR \"now after so many years\"'\n",
    "not_wanted = '-#porn -#horny -#cancer -#breastcancer -#bullying '\n",
    "tweet_specifications = '-has:links -is:retweet  -has:media -has:images -has:video_link '\n",
    "geolocation = '(place_country:US OR place_country:GB) '\n",
    "query_text = lang + ' (' + key_hashtags + ' OR ' + questions + ') ' + not_wanted + tweet_specifications #+ geolocation \n",
    "query_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fdb8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#endpoint\n",
    "endpoint = \"https://api.twitter.com/2/tweets/search/all/\"\n",
    "\n",
    "# Max results per request\n",
    "max_results = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b05c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of tweets for the period of time\n",
    "# Set period of time\n",
    "start_time = \"2017-07-01T00:00:00Z\"\n",
    "end_time = \"2017-10-01T00:00:00Z\"\n",
    "results, _, count = get_data_counts(query_text, start_time, end_time, \"\", \"https://api.twitter.com/2/tweets/counts/all\")\n",
    "print('Tweets in 2017:', results)\n",
    "\n",
    "# Set period of time\n",
    "start_time = \"2018-07-01T00:00:00Z\"\n",
    "end_time = \"2018-10-01T00:00:00Z\"\n",
    "results, _, count = get_data_counts(query_text, start_time, end_time, \"\", \"https://api.twitter.com/2/tweets/counts/all\")\n",
    "print('Tweets in 2018:', results)\n",
    "\n",
    "# Set period of time\n",
    "start_time = \"2022-07-01T00:00:00Z\"\n",
    "end_time = \"2022-10-01T00:00:00Z\"\n",
    "results, _, count = get_data_counts(query_text, start_time, end_time, \"\", \"https://api.twitter.com/2/tweets/counts/all\")\n",
    "print('Tweets in 2022:', results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081c3fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set period of time and download tweets\n",
    "start_time = \"2017-07-01T00:00:00Z\"\n",
    "end_time = \"2017-10-01T00:00:00Z\"\n",
    "tweets = get_data(query_text, start_time=start_time, end_time=end_time, \n",
    "                  max_results=max_results, expansions='author_id,in_reply_to_user_id,geo.place_id',\n",
    "                  tweet_fields='id,text,author_id,in_reply_to_user_id,geo,conversation_id,created_at,lang,public_metrics,referenced_tweets,reply_settings,source,entities',\n",
    "                  user_fields='id,name,username,created_at,description,public_metrics,verified',\n",
    "                  place_fields='full_name,id,country,country_code,geo,name,place_type',\n",
    "                  endpoint= endpoint,\n",
    "                  next_token='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788f7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "df_tweets = pd.DataFrame(tweets)\n",
    "df_tweets.to_pickle(path+\"tweets2017.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509c853e",
   "metadata": {},
   "source": [
    "## Sample 10% of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27db0a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df = pd.read_pickle(path+\"tweets2022.pkl\")\n",
    "sampled_tweets = tweets_df.sample(frac=0.1, replace=False, random_state=14538, axis=0, ignore_index=True)\n",
    "sampled_tweets.to_pickle(path+\"sampled_tweets2022.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f970df8b",
   "metadata": {},
   "source": [
    "## Extract clean text from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285cbf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "mocktext = 'Hey Anna!! Have u heard???? @alessio_123 \"has been\" #reported *for #abuse-d & misconduct https://es.wikipedia.org (1)'\n",
    "print('Mock tweet')\n",
    "print(mocktext)\n",
    "print('Clean text')\n",
    "print(tweet2text(mocktext))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed2965c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to our data\n",
    "#Load tweets\n",
    "tweets_df = pd.read_pickle(path+\"sampled_tweets2022.pkl\")\n",
    "\n",
    "# Create df to store results and without irrelevant data\n",
    "tweets_filtered = tweets_df.copy() \n",
    "column_list = [\"id\", \"text\"]\n",
    "tweets_filtered = tweets_filtered[column_list]\n",
    "\n",
    "# Clean text\n",
    "tweets_filtered[\"clean_text\"] = tweets_filtered[\"text\"].map(tweet2text)\n",
    "tweets_filtered.loc[tweets_filtered[\"clean_text\"].isnull(),\"clean_text\"] = \"\"\n",
    "\n",
    "# Save\n",
    "tweets_filtered.to_csv(path+\"clean_tweets2022.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1461124",
   "metadata": {},
   "source": [
    "## Build networks - nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d05992e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tweets\n",
    "tweets_df = pd.read_pickle(path+\"sampled_tweets2022.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adc4684",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"extra_clean_text\"] = tweets[\"text\"].map(cleaner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f77bf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = nltk.TweetTokenizer()\n",
    "\n",
    "#initialize an empty dict\n",
    "unique_words = {}\n",
    "\n",
    "for idx, row in tweets.iterrows():\n",
    "    if row[\"extra_clean_text\"] != \"\":\n",
    "        for word in tweet_tokenizer.tokenize(row[\"extra_clean_text\"]):\n",
    "            unique_words.setdefault(word,0)\n",
    "            unique_words[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69910de1",
   "metadata": {},
   "source": [
    "### Words + emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8089ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_tokenizer = nltk.TweetTokenizer()\n",
    "\n",
    "#initialize an empty dict\n",
    "unique_words = {}\n",
    "words_anger = {}\n",
    "words_sad = {}\n",
    "words_tone = {}\n",
    "words_posemo = {}\n",
    "words_negemo = {}\n",
    "words_anx = {}\n",
    "like_count = {}\n",
    "tweets_and_likes = 0\n",
    "\n",
    "for idx, row in tweets.iterrows():\n",
    "    if row[\"extra_clean_text\"] != \"\":\n",
    "        tweet_likes = tweets_data['public_metrics'][2]['like_count'] +1 # we add the +1 for the tweet itself\n",
    "        tweets_and_likes += tweet_likes # will be used for averaging\n",
    "        for word in tweet_tokenizer.tokenize(row[\"extra_clean_text\"]):\n",
    "            # if word hasnt appeared before, create a key-value pair\n",
    "            unique_words.setdefault(word,0)\n",
    "            words_anger.setdefault(word,0)\n",
    "            words_sad.setdefault(word,0)\n",
    "            words_tone.setdefault(word,0)\n",
    "            words_posemo.setdefault(word,0)\n",
    "            words_negemo.setdefault(word,0)\n",
    "            words_anx.setdefault(word,0)\n",
    "            like_count.setdefault(word,0)\n",
    "            # Count words appearence\n",
    "            unique_words[word] += 1\n",
    "            # Take into account likes and add the emotions\n",
    "            words_anger[word] += tweet_likes*row['anger']\n",
    "            words_sad[word] += tweet_likes*row['sad']\n",
    "            words_tone[word] += tweet_likes*row['Tone']\n",
    "            words_posemo[word] += tweet_likes*row['posemo']\n",
    "            words_negemo[word] += tweet_likes*row['negemo']\n",
    "            words_anx[word] += tweet_likes*row['anx']\n",
    "            like_count[word] += tweet_likes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1bdae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn to dfs\n",
    "uw_df = pd.DataFrame.from_dict(unique_words, orient='index').reset_index()\n",
    "anger_df = pd.DataFrame.from_dict(words_anger, orient='index').reset_index()\n",
    "sad_df = pd.DataFrame.from_dict(words_sad, orient='index').reset_index()\n",
    "posemo_df = pd.DataFrame.from_dict(words_posemo, orient='index').reset_index()\n",
    "negemo_df = pd.DataFrame.from_dict(words_negemo, orient='index').reset_index()\n",
    "anx_df = pd.DataFrame.from_dict(words_anx, orient='index').reset_index()\n",
    "tone_df = pd.DataFrame.from_dict(words_tone, orient='index').reset_index()\n",
    "likes_df = pd.DataFrame.from_dict(like_count, orient='index').reset_index()\n",
    "# Rename columns\n",
    "uw_df.rename(columns = {'index':'Word', 0:'count'}, inplace=True)\n",
    "anger_df.rename(columns = {'index':'Word', 0:'anger'}, inplace=True)\n",
    "sad_df.rename(columns = {'index':'Word', 0:'sad'}, inplace=True)\n",
    "posemo_df.rename(columns = {'index':'Word', 0:'posemo'}, inplace=True)\n",
    "negemo_df.rename(columns = {'index':'Word', 0:'negemo'}, inplace=True)\n",
    "anx_df.rename(columns = {'index':'Word', 0:'anx'}, inplace=True)\n",
    "tone_df.rename(columns = {'index':'Word', 0:'tone'}, inplace=True)\n",
    "likes_df.rename(columns = {'index':'Word', 0:'total_counts'}, inplace=True) # tweets+favs\n",
    "\n",
    "# Combine to one dataframe\n",
    "words_df = uw_df.join(anger_df.set_index('Word'), on='Word')\n",
    "words_df = words_df.join(sad_df.set_index('Word'), on='Word')\n",
    "words_df = words_df.join(anx_df.set_index('Word'), on='Word')\n",
    "words_df = words_df.join(posemo_df.set_index('Word'), on='Word')\n",
    "words_df = words_df.join(negemo_df.set_index('Word'), on='Word')\n",
    "words_df = words_df.join(tone_df.set_index('Word'), on='Word')\n",
    "words_df = words_df.join(likes_df.set_index('Word'), on='Word')\n",
    "words_df.sort_values(by=['count'], ascending=False, inplace=True)\n",
    "words_df = words_df.reset_index().drop(columns=[\"index\"])\n",
    "words_df['anger'] = words_df['anger']/words_df['total_counts']\n",
    "words_df['sad'] = words_df['sad']/words_df['total_counts']\n",
    "words_df['posemo'] = words_df['posemo']/words_df['total_counts']\n",
    "words_df['negemo'] = words_df['negemo']/words_df['total_counts']\n",
    "words_df['anx'] = words_df['anx']/words_df['total_counts']\n",
    "words_df['tone'] = words_df['tone']/words_df['total_counts']\n",
    "words_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb2cafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_df['emotion'] = words_df[['posemo', 'negemo']].idxmax(axis=1)\n",
    "words_df['negative_emotion'] = words_df[['anger', 'sad', 'anx']].idxmax(axis=1)\n",
    "words_df['main_emotion'] = np.where(words_df['emotion'] == 'negemo', words_df['negative_emotion'], 'posemo')\n",
    "words_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225a44ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_df = words_df[['Word', 'count', 'main_emotion']]\n",
    "emotions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eecaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "uw_df.to_csv(path+\"words_2022.csv\")\n",
    "words_df.to_csv(path+\"words_emotionvalues2022.csv\")\n",
    "emotions_df.to_csv(path+\"words_mainemotion2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97399c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "uw = unique_words.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f463ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = {}\n",
    "network_key = 0\n",
    "for index, row in tweets.iterrows():\n",
    "    combined_list = [word for word in str.split(row[\"clean_text\"], \" \") if word in uw]\n",
    "    #itertool product creates Cartesian product of each element in the combined list\n",
    "    for pair in itertools.product(combined_list, combined_list):\n",
    "        #exclude self-loops and count each pair only once because our graph is undirected and we do not take self-loops into account\n",
    "        if pair[0]!=pair[1] and not(pair[::-1] in network):\n",
    "            network.setdefault(pair,0)\n",
    "            network[pair] += 1 \n",
    "    \n",
    "network_df = pd.DataFrame.from_dict(network, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6557e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df.reset_index(inplace=True)\n",
    "network_df.columns = [\"pair\",\"weight\"]\n",
    "network_df.sort_values(by=\"weight\",inplace=True, ascending=False)\n",
    "network_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c740d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get weighted graph we need a list of 3-element tuplels (u,v,w) where u and v are nodes and w is a number representing weight\n",
    "up_weighted = []\n",
    "for edge in network:\n",
    "    #we can filter edges by weight by uncommenting the next line and setting desired weight threshold\n",
    "    #if(network[edge])>1:\n",
    "    up_weighted.append((edge[0],edge[1],network[edge]))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(up_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8429c190",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(G.nodes()))\n",
    "print(len(G.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f957c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = path+\"edgelist_words_2022.csv\"\n",
    "nx.write_weighted_edgelist(G, filename, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996bc389",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_nodes = pd.DataFrame.from_dict(unique_words,orient=\"index\")\n",
    "word_nodes.reset_index(inplace=True)\n",
    "word_nodes[\"Label\"] = word_nodes[\"index\"]\n",
    "word_nodes.rename(columns={\"index\":\"Id\",0:\"delete\"},inplace=True)\n",
    "word_nodes = word_nodes.drop(columns=['delete'])\n",
    "\n",
    "word_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0064787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_nodes.to_csv(path+\"nodelist_words_2022.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57774db4",
   "metadata": {},
   "source": [
    "### Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a0efa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.loc[tweets[\"entities\"].isnull(), \"entities\"] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca935b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"hashtags\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c18b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_hashtags = {}\n",
    "index = 0\n",
    "\n",
    "for idx, row in tweets.iterrows():\n",
    "    if row[\"entities\"] is not None and \"hashtags\" in row[\"entities\"]:\n",
    "        hl = []\n",
    "        for hashtag in row[\"entities\"][\"hashtags\"]:\n",
    "            tag = hashtag[\"tag\"].lower()\n",
    "            unique_hashtags.setdefault(tag, 0)\n",
    "            unique_hashtags[tag] += 1\n",
    "            hl.append(tag)\n",
    " \n",
    "        tweets.at[idx,\"hashtags\"] = hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6515ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_hashtags = dict(sorted(unique_hashtags.items(), key=lambda item: item[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95420eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "uh_df = pd.DataFrame.from_dict(unique_hashtags, orient='index').reset_index()\n",
    "uh_df.rename(columns = {'index':'Hashtag', 0:'Count'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd41a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "uh_df[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d4ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "uh_df.to_csv(path+\"hashtags_2022.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e16e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "uh = unique_hashtags.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a10c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = {}\n",
    "network_key = 0\n",
    "for index, row in tweets.iterrows():\n",
    "    combined_list = [hashtag for hashtag in row[\"hashtags\"]]\n",
    "    #itertool product creates Cartesian product of each element in the combined list\n",
    "    for pair in itertools.product(combined_list, combined_list):\n",
    "        #exclude self-loops and count each pair only once because our graph is undirected and we do not take self-loops into account\n",
    "        if pair[0]!=pair[1] and not(pair[::-1] in network):\n",
    "            network.setdefault(pair,0)\n",
    "            network[pair] += 1 \n",
    "    \n",
    "network_df = pd.DataFrame.from_dict(network, orient=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2f27df",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df.reset_index(inplace=True)\n",
    "network_df.columns = [\"pair\",\"weight\"]\n",
    "network_df.sort_values(by=\"weight\",inplace=True, ascending=False)\n",
    "network_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9b0c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get weighted graph we need a list of 3-element tuplels (u,v,w) where u and v are nodes and w is a number representing weight\n",
    "up_weighted = []\n",
    "for edge in network:\n",
    "    #we can filter edges by weight by uncommenting the next line and setting desired weight threshold\n",
    "    #if(network[edge])>1:\n",
    "    up_weighted.append((edge[0],edge[1],network[edge]))\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_weighted_edges_from(up_weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25bc75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(G.nodes()))\n",
    "print(len(G.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d417ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = path+\"edgelist_hashtags_2022.csv\"\n",
    "nx.write_weighted_edgelist(G, filename, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521c6417",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_nodes = uh_df.copy()\n",
    "hashtag_nodes[\"Label\"] = hashtag_nodes[\"Hashtag\"]\n",
    "hashtag_nodes.rename(columns={\"Hashtag\":\"Id\"},inplace=True)\n",
    "hashtag_nodes = hashtag_nodes.drop(columns=['Count'])\n",
    "hashtag_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebe9d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_nodes.to_csv(path+\"nodelist_hashtags_2022.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccf7f39",
   "metadata": {},
   "source": [
    "## Extracting main co-occurrences with the word victim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9cdb48",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2017 = pd.read_csv('edgelist_words_2017.csv', names=['word1', 'word2', 'count'])\n",
    "victim_2017 = words_2017[words_2017.apply(lambda x: x.astype(str).str.contains('victim').any(), axis=1)]\n",
    "#victim_2017 = victim_2017.sort_values('count', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47386b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2018 = pd.read_csv('edgelist_words_2018.csv', names=['word1', 'word2', 'count'])\n",
    "victim_2018 = words_2018[words_2018.apply(lambda x: x.astype(str).str.contains('victim').any(), axis=1)]\n",
    "#victim_2018 = victim_2018.sort_values('count', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7802001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_2022 = pd.read_csv('edgelist_words_2022.csv', names=['word1', 'word2', 'count'])\n",
    "victim_2022 = words_2022[words_2022.apply(lambda x: x.astype(str).str.contains('victim').any(), axis=1)]\n",
    "#victim_2022 = victim_2022.sort_values('count', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf93572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_related(df):\n",
    "    col1 = df[df.apply(lambda x: x['word1'] != 'victim', axis=1)][['word1', 'count']]\n",
    "    col1 = col1.rename(columns={'word1': 'word'})\n",
    "    col2 = df[df.apply(lambda x: x['word2'] != 'victim', axis=1)][['word2', 'count']]\n",
    "    col2 = col2.rename(columns={'word2': 'word'})\n",
    "    result = pd.concat([col1,col2])\n",
    "    result = result.groupby('word', as_index=False).max()\n",
    "    return result.sort_values('count', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf3dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "relatedwords17.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1389b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "relatedwords18.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242e23b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "relatedwords22.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31458c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = relatedwords17\n",
    "df2 = relatedwords18\n",
    "df3 = relatedwords22\n",
    "\n",
    "# Get the top 5 words for each dataframe\n",
    "df1_top_5 = df1.head(5)\n",
    "df2_top_5 = df2.head(5)\n",
    "df3_top_5 = df3.head(5)\n",
    "\n",
    "# Get the words from each dataframe\n",
    "df1_words = df1_top_5['word'].values\n",
    "df2_words = df2_top_5['word'].values\n",
    "df3_words = df3_top_5['word'].values\n",
    "\n",
    "# Get the common words between dataframes\n",
    "common_words = set(df1_words).intersection(df2_words, df3_words)\n",
    "\n",
    "# get not common words\n",
    "df1_not = set(df1_words) - common_words\n",
    "df2_not = set(df2_words) - common_words\n",
    "df3_not = set(df3_words) - common_words\n",
    "\n",
    "# Get the words that are not in the top 5 for each dataframe\n",
    "df1_not_in_top_5 = set(df1_words) - set(df2_words) - set(df3_words)\n",
    "df2_not_in_top_5 = set(df2_words) - set(df1_words) - set(df3_words)\n",
    "df3_not_in_top_5 = set(df3_words) - set(df1_words) - set(df2_words)\n",
    "\n",
    "# Get the words to keep for each dataframe\n",
    "df1_words_to_keep = list(df1_words) + list(df2_not) + list(df3_not)\n",
    "df2_words_to_keep = list(df2_words) + list(df1_not) + list(df3_not)\n",
    "df3_words_to_keep = list(df3_words) + list(df1_not) + list(df2_not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e74c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to indicate which dataframe they come from\n",
    "# Filter each dataframe to only keep the words to keep\n",
    "df1 = df1[df1['word'].isin(df1_words_to_keep)].rename(columns={'count': '2017'})\n",
    "df2 = df2[df2['word'].isin(df2_words_to_keep)].rename(columns={'count': '2018'})\n",
    "df3 = df3[df3['word'].isin(df3_words_to_keep)].rename(columns={'count': '2022'})\n",
    "\n",
    "\n",
    "# Merge dataframes\n",
    "result = pd.merge(df1, df2, on='word', how='outer')\n",
    "result = pd.merge(result, df3, on='word', how='outer')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76703cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with 0\n",
    "result = result.fillna(0)\n",
    "colors = ['slateblue', 'coral', 'orange','red', 'orchid', 'pink', 'lightblue', 'green', 'palevioletred', 'navy', 'firebrick','darkgray', 'lightgreen']\n",
    "\n",
    "# Plot data\n",
    "for word, color in zip(result['word'], colors):\n",
    "    x = [1, 2, 6]\n",
    "    y = result.loc[result['word'] == word, result.columns[1:]].values[0]\n",
    "    plt.plot(x, y, marker='o', linestyle='-', label=word, color = color)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)\n",
    "plt.xticks(x, result.columns[1:])\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Appearences with the word \"victim\"')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
